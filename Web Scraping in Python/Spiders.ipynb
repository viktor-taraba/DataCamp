{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spiders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Inheriting the Spider**\n",
        "When learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\n",
        "\n",
        "As mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\n",
        "\n",
        "We wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!"
      ],
      "metadata": {
        "id": "1vtzEAP4ezbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VbWOOOuOesYl"
      },
      "outputs": [],
      "source": [
        "def inspect_class(c):\n",
        "  newc = c()\n",
        "  meths = dir(newc)\n",
        "  if 'name' in meths:\n",
        "    print(\"Your spider class name is:\", newc.name)\n",
        "  if 'from_crawler' in meths:\n",
        "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
        "  else:\n",
        "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scrapy library\n",
        "import scrapy\n",
        "\n",
        "# Create the spider class\n",
        "class YourSpider(scrapy.Spider):\n",
        "  name = \"your_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests(self):\n",
        "    pass\n",
        "  # parse method\n",
        "  def parse(self, response):\n",
        "    pass\n",
        "  \n",
        "# Inspect Your Class\n",
        "inspect_class(YourSpider)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyj2YhiYe8tn",
        "outputId": "e60d61e2-7c55-41a5-b92d-6cb3f7755971"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your spider class name is: your_spider\n",
            "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hurl the URLs**\n",
        "In the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\n",
        "\n",
        "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\n",
        "\n",
        "Note: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end."
      ],
      "metadata": {
        "id": "kgl_L9EPfIXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_class( c ):\n",
        "  newc = c()\n",
        "  meths = dir( newc )\n",
        "  if 'start_requests' in meths:\n",
        "    print( \"The start_requests method yields the following urls:\" )\n",
        "    for u in newc.start_requests():\n",
        "      print(  \"\\t-\", u )"
      ],
      "metadata": {
        "id": "MUGvtQg1fYSf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scrapy library\n",
        "import scrapy\n",
        "\n",
        "# Create the spider class\n",
        "class YourSpider( scrapy.Spider ):\n",
        "  name = \"your_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
        "    for url in urls:\n",
        "      yield url\n",
        "  # parse method\n",
        "  def parse( self, response ):\n",
        "    pass\n",
        "  \n",
        "# Inspect Your Class\n",
        "inspect_class( YourSpider )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc5DRJbbe-ek",
        "outputId": "1569692d-51d8-4b60-99be-2e334ca30eff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The start_requests method yields the following urls:\n",
            "\t- https://www.datacamp.com\n",
            "\t- https://scrapy.org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Self Referencing is Classy**\n",
        "You probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\n",
        "\n",
        "In this exercise you will get a chance to play with this \"self referencing\"."
      ],
      "metadata": {
        "id": "r0ZXd-u1ffVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_class( c ):\n",
        "  newc = c()\n",
        "  try:\n",
        "    newc.start_requests()\n",
        "  except:\n",
        "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
      ],
      "metadata": {
        "id": "0YEJwSlUfrNl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the spider class\n",
        "class YourSpider( scrapy.Spider ):\n",
        "  name = \"your_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    self.print_msg( \"Hello World!\" )\n",
        "  # parse method\n",
        "  def parse( self, response ):\n",
        "    pass\n",
        "  # print_msg method\n",
        "  def print_msg( self, msg ):\n",
        "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
        "  \n",
        "# Inspect Your Class\n",
        "inspect_class( YourSpider )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDMuf70BfQBf",
        "outputId": "1f6a4039-1172-429a-a85b-085957cd01b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling start_requests in YourSpider prints out: Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Starting with Start Requests**\n",
        "In the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\n",
        "\n",
        "As before, we have created the function inspect_class to examine what you are yielding in start_requests."
      ],
      "metadata": {
        "id": "tr3AJDJ1fvky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_class( c ):\n",
        "  newc = c()\n",
        "  try:\n",
        "    y = list( newc.start_requests() )\n",
        "    first_yield = y[0]\n",
        "    print( \"The url you would scrape is:\", first_yield.url )\n",
        "    cb = first_yield.callback\n",
        "    print( \"The name of the callback method you called is:\", cb.__name__ )\n",
        "  except:\n",
        "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
      ],
      "metadata": {
        "id": "nkGJfNJJfrpW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the spider class\n",
        "class YourSpider( scrapy.Spider ):\n",
        "  name = \"your_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
        "  # parse method\n",
        "  def parse( self, response ):\n",
        "    pass\n",
        "  \n",
        "# Inspect Your Class\n",
        "inspect_class( YourSpider )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ErRdPlofyRT",
        "outputId": "74eae6d3-9189-48d7-81c0-0764cc2a2ace"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The url you would scrape is: https://www.datacamp.com\n",
            "The name of the callback method you called is: parse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Pen Names**\n",
        "In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\n",
        "\n",
        "Two things you should know:\n",
        "\n",
        "* You will be using the response object and the css method here.\n",
        "* The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name"
      ],
      "metadata": {
        "id": "T25iwo5Mf64N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
        "\n",
        "\n",
        "# Create the Spider class\n",
        "class DCspider( scrapy.Spider ):\n",
        "  name = 'dcspider'\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
        "  # parse method\n",
        "  def parse( self, response):\n",
        "    # Create an extracted list of course author names\n",
        "    author_names = response.css('p.course-block__author-name ::text').extract()\n",
        "    # Here we will just return the list of Authors\n",
        "    return author_names"
      ],
      "metadata": {
        "id": "MyCHygg4f5q1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Crawler Time**\n",
        "This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!"
      ],
      "metadata": {
        "id": "A_0Y-QPghKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Spider class\n",
        "class DCdescr( scrapy.Spider ):\n",
        "  name = 'dcdescr'\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
        "  \n",
        "  # First parse method\n",
        "  def parse( self, response ):\n",
        "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
        "    # Follow each of the extracted links\n",
        "    for link in links:\n",
        "      yield response.follow( url = link, callback = self.parse_descr )\n",
        "      \n",
        "  # Second parsing method\n",
        "  def parse_descr( self, response ):\n",
        "    # Extract course description\n",
        "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
        "    # For now, just yield the course description\n",
        "    yield course_descr"
      ],
      "metadata": {
        "id": "-no2h8dLgKkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Time to Run**\n",
        "In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n",
        "\n",
        "The point of this exercise is to remedy that!\n",
        "\n",
        "The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!"
      ],
      "metadata": {
        "id": "PdO2cC6xhdKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def previewCourses( dc_dict, n = 3 ):\n",
        "  crs_titles = list( dc_dict.keys() )\n",
        "  print( \"A preview of DataCamp Courses:\")\n",
        "  print(\"---------------------------------------\\n\")\n",
        "  for t in crs_titles[:n]:\n",
        "    print( \"TITLE: %s\" % t)\n",
        "    for i,ct in enumerate(dc_dict[t]):\n",
        "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "zRIxHIBkhod1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scrapy\n",
        "import scrapy\n",
        "\n",
        "# Import the CrawlerProcess: for running the spider\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
        "\n",
        "# Create the Spider class\n",
        "class DC_Chapter_Spider(scrapy.Spider):\n",
        "  name = \"dc_chapter_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests(self):\n",
        "    yield scrapy.Request(url = url_short,\n",
        "                         callback = self.parse_front)\n",
        "  # First parsing method\n",
        "  def parse_front(self, response):\n",
        "    course_blocks = response.css('div.course-block')\n",
        "    course_links = course_blocks.xpath('./a/@href')\n",
        "    links_to_follow = course_links.extract()\n",
        "    for url in links_to_follow:\n",
        "      yield response.follow(url = url,\n",
        "                            callback = self.parse_pages)\n",
        "  # Second parsing method\n",
        "  def parse_pages(self, response):\n",
        "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "    ch_titles = response.css('h4.chapter__title::text')\n",
        "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
        "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
        "\n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(DC_Chapter_Spider)\n",
        "process.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD8lqtxVhZio",
        "outputId": "12e3bace-3187-4c03-de5e-f019b3226062"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-28 19:34:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
            "2022-04-28 19:34:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.2 15 Mar 2022), cryptography 37.0.1, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-04-28 19:34:53 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2022-04-28 19:34:53 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-04-28 19:34:53 [scrapy.extensions.telnet] INFO: Telnet Password: 2ec7baeb457b69a0\n",
            "2022-04-28 19:34:53 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-04-28 19:34:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-04-28 19:34:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-04-28 19:34:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-04-28 19:34:53 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-04-28 19:34:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-04-28 19:34:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-04-28 19:34:53 [filelock] DEBUG: Attempting to acquire lock 140205261862544 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.2.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-04-28 19:34:53 [filelock] DEBUG: Lock 140205261862544 acquired on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.2.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-04-28 19:34:53 [filelock] DEBUG: Attempting to release lock 140205261862544 on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.2.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-04-28 19:34:53 [filelock] DEBUG: Lock 140205261862544 released on /root/.cache/python-tldextract/3.7.13.final__usr__7d8fdf__tldextract-3.2.1/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
            "2022-04-28 19:34:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-04-28 19:34:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 7020,\n",
            " 'downloader/request_count': 12,\n",
            " 'downloader/request_method_count/GET': 12,\n",
            " 'downloader/response_bytes': 2465114,\n",
            " 'downloader/response_count': 12,\n",
            " 'downloader/response_status_count/200': 11,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 1.168808,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 4, 28, 19, 34, 54, 739559),\n",
            " 'httpcompression/response_bytes': 1001,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/404': 1,\n",
            " 'log_count/DEBUG': 17,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 131682304,\n",
            " 'memusage/startup': 131682304,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 12,\n",
            " 'scheduler/dequeued': 12,\n",
            " 'scheduler/dequeued/memory': 12,\n",
            " 'scheduler/enqueued': 12,\n",
            " 'scheduler/enqueued/memory': 12,\n",
            " 'start_time': datetime.datetime(2022, 4, 28, 19, 34, 53, 570751)}\n",
            "2022-04-28 19:34:54 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previewCourses(dc_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbpKmEzThrM7",
        "outputId": "989b8487-0cb7-4f5b-c172-600ef088ad12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A preview of DataCamp Courses:\n",
            "---------------------------------------\n",
            "\n",
            "TITLE: Intermediate R\n",
            "\tChapter 1: Conditionals and Control Flow\n",
            "\tChapter 2: Functions\n",
            "\tChapter 3: Utilities\n",
            "\tChapter 4: Loops\n",
            "\tChapter 5: The apply family\n",
            "\tChapter 6: Conditionals and Control Flow\n",
            "\tChapter 7: Loops\n",
            "\tChapter 8: Functions\n",
            "\tChapter 9: The apply family\n",
            "\tChapter 10: Utilities\n",
            "\n",
            "TITLE: Data Analysis in R, the data.table Way\n",
            "\tChapter 1: Data.table novice\n",
            "\tChapter 2: Data.table yeoman\n",
            "\tChapter 3: Data.table expert\n",
            "\n",
            "TITLE: Reporting with R Markdown\n",
            "\tChapter 1: Authoring R Markdown Reports\n",
            "\tChapter 2: Embedding Code\n",
            "\tChapter 3: Compiling Reports\n",
            "\tChapter 4: Configuring R Markdown (optional)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DataCamp Descriptions**\n",
        "Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\n",
        "\n",
        "As in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\n",
        "\n",
        "In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines)."
      ],
      "metadata": {
        "id": "iu1QfzVPq5Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def previewCourses( dc_dict, n = 1 ):\n",
        "  crs_titles = list( dc_dict.keys() )\n",
        "  print( \"A preview of DataCamp Courses:\")\n",
        "  print(\"---------------------------------------\\n\")\n",
        "  for t in crs_titles[:n]:\n",
        "    print( \"TITLE: %s\" % t)\n",
        "    print(\"\\tDescription: %s\" % dc_dict[t] )\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "w8F-09JBs8Cn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scrapy\n",
        "import scrapy\n",
        "import sys     \n",
        "\n",
        "# Import the CrawlerProcess: for running the spider\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Create the Spider class\n",
        "class DC_Description_Spider(scrapy.Spider):\n",
        "  name = \"dc_chapter_spider\"\n",
        "  # start_requests method\n",
        "  def start_requests(self):\n",
        "    yield scrapy.Request(url = url_short,\n",
        "                         callback = self.parse_front)\n",
        "  # First parsing method\n",
        "  def parse_front(self, response):\n",
        "    course_blocks = response.css('div.course-block')\n",
        "    course_links = course_blocks.xpath('./a/@href')\n",
        "    links_to_follow = course_links.extract()\n",
        "    for url in links_to_follow:\n",
        "      yield response.follow(url = url,\n",
        "                            callback = self.parse_pages)\n",
        "  # Second parsing method\n",
        "  def parse_pages(self, response):\n",
        "    # Create a SelectorList of the course titles text\n",
        "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
        "    # Extract the text and strip it clean\n",
        "    crs_title_ext = crs_title.extract_first().strip()\n",
        "    # Create a SelectorList of course descriptions text\n",
        "    crs_descr = response.css( 'p.course__description ::text' )\n",
        "    # Extract the text and strip it clean\n",
        "    crs_descr_ext = crs_descr.extract_first().strip()\n",
        "    # Fill in the dictionary\n",
        "    dc_dict[crs_title_ext] = crs_descr_ext\n",
        "\n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "if \"twisted.internet.reactor\" in sys.modules: \n",
        "  del sys.modules[\"twisted.internet.reactor\"]\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(DC_Description_Spider)\n",
        "process.start()"
      ],
      "metadata": {
        "id": "yphkheEeh0-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54341d8-8370-4a2b-a618-f06de3b685e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-28 19:44:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
            "2022-04-28 19:44:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.2 15 Mar 2022), cryptography 37.0.1, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-04-28 19:44:14 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2022-04-28 19:44:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-04-28 19:44:14 [scrapy.extensions.telnet] INFO: Telnet Password: 189b4a6e610689e4\n",
            "2022-04-28 19:44:14 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-04-28 19:44:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-04-28 19:44:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-04-28 19:44:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-04-28 19:44:14 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-04-28 19:44:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-04-28 19:44:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-04-28 19:44:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
            "2022-04-28 19:44:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-04-28 19:44:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 7020,\n",
            " 'downloader/request_count': 12,\n",
            " 'downloader/request_method_count/GET': 12,\n",
            " 'downloader/response_bytes': 2465118,\n",
            " 'downloader/response_count': 12,\n",
            " 'downloader/response_status_count/200': 11,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 0.905351,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 4, 28, 19, 44, 15, 527568),\n",
            " 'httpcompression/response_bytes': 1001,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'httperror/response_ignored_count': 1,\n",
            " 'httperror/response_ignored_status_count/404': 1,\n",
            " 'log_count/DEBUG': 13,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 175452160,\n",
            " 'memusage/startup': 175452160,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 12,\n",
            " 'scheduler/dequeued': 12,\n",
            " 'scheduler/dequeued/memory': 12,\n",
            " 'scheduler/enqueued': 12,\n",
            " 'scheduler/enqueued/memory': 12,\n",
            " 'start_time': datetime.datetime(2022, 4, 28, 19, 44, 14, 622217)}\n",
            "2022-04-28 19:44:15 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a preview of courses\n",
        "previewCourses(dc_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1wKqiour8t6",
        "outputId": "10a7e0f7-6f19-41c0-c391-137ab0f36c8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A preview of DataCamp Courses:\n",
            "---------------------------------------\n",
            "\n",
            "TITLE: Intro to Python for Data Science\n",
            "\tDescription: Python is a general-purpose programming language that is becoming more and more popular for doing data science. Companies worldwide are using Python to harvest insights from their data and get a competitive edge. Unlike any other Python tutorial, this course focuses on Python specifically for data science. In our Intro to Python class, you will learn about powerful ways to store and manipulate data as well as cool data science tools to start your own analyses. Enter DataCampâ€™s online Python curriculum.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Capstone Crawler**\n",
        "This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\n",
        "\n",
        "* The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\n",
        "* The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline)."
      ],
      "metadata": {
        "id": "BAU8REsMtDJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def previewCourses( dc_dict = dc_dict, n = 3 ):\n",
        "  crs_titles = list( dc_dict.keys() )\n",
        "  print( \"A preview of DataCamp Courses:\")\n",
        "  print(\"---------------------------------------\\n\")\n",
        "  for t in crs_titles[:n]:\n",
        "    print( \"TITLE: %s\" % t)\n",
        "    print( \"\\tDESCRIPTION: %s\" % dc_dict[t] )\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "DIUlkp8vtKku"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scrapy\n",
        "import scrapy\n",
        "\n",
        "# Import the CrawlerProcess\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Create the Spider class\n",
        "class YourSpider( scrapy.Spider ):\n",
        "  name = 'yourspider'\n",
        "  # start_requests method\n",
        "  def start_requests( self ):\n",
        "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
        "      \n",
        "  def parse(self, response):\n",
        "    # My version of the parser you wrote in the previous part\n",
        "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
        "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
        "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
        "      dc_dict[crs_title] = crs_descr\n",
        "    \n",
        "# Initialize the dictionary **outside** of the Spider class\n",
        "dc_dict = dict()\n",
        "\n",
        "# Run the Spider\n",
        "if \"twisted.internet.reactor\" in sys.modules: \n",
        "  del sys.modules[\"twisted.internet.reactor\"]\n",
        "process = CrawlerProcess()\n",
        "process.crawl(YourSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1K8j28IsqNg",
        "outputId": "8fe001c3-431d-42d8-e2cd-c3e10940579a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-28 19:47:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
            "2022-04-28 19:47:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.2 15 Mar 2022), cryptography 37.0.1, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-04-28 19:47:40 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2022-04-28 19:47:40 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-04-28 19:47:40 [scrapy.extensions.telnet] INFO: Telnet Password: eca1afbad0829fd9\n",
            "2022-04-28 19:47:40 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-04-28 19:47:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-04-28 19:47:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-04-28 19:47:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-04-28 19:47:40 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-04-28 19:47:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-04-28 19:47:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-04-28 19:47:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
            "2022-04-28 19:47:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-04-28 19:47:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 307,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 171632,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.345363,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 4, 28, 19, 47, 41, 239412),\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 176521216,\n",
            " 'memusage/startup': 176521216,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 4, 28, 19, 47, 40, 894049)}\n",
            "2022-04-28 19:47:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a preview of courses\n",
        "previewCourses(dc_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl6zZ9ABtUgG",
        "outputId": "3c677caa-62cc-494a-c736-72ac9c7bf8b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A preview of DataCamp Courses:\n",
            "---------------------------------------\n",
            "\n",
            "TITLE: Introduction to R\n",
            "\tDESCRIPTION: \n",
            "          Master the basics of data analysis by manipulating common data structures such as vectors, matrices and data frames.\n",
            "        \n",
            "\n",
            "TITLE: Data Analysis in R, the data.table Way\n",
            "\tDESCRIPTION: \n",
            "          Master core concepts in data manipulation such as subsetting, updating, indexing and joining your data using data.table.\n",
            "        \n",
            "\n",
            "TITLE: Data Manipulation in R with dplyr\n",
            "\tDESCRIPTION: \n",
            "          Master techniques for data manipulation using the select, mutate, filter, arrange, and summarise functions in dplyr.\n",
            "        \n",
            "\n"
          ]
        }
      ]
    }
  ]
}